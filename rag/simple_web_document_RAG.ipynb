{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vacayy/ai-playground/blob/main/rag/simple_web_document_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwaANvAGIwl4"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-community langchain-chroma langchain-openai bs4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4  # BeautifulSoup4: HTML 및 XML 파일을 파싱하기 위한 라이브러리\n",
        "from langchain import hub  # LangChain의 다양한 유틸리티와 허브 사용\n",
        "from langchain_chroma import Chroma  # 벡터 데이터 저장소인 Chroma 사용\n",
        "from langchain_openai import ChatOpenAI  # OpenAI LLM 모델 사용\n",
        "from langchain_openai import OpenAIEmbeddings  # OpenAI 임베딩 사용\n",
        "from langchain_community.document_loaders import WebBaseLoader  # 웹 문서를 불러오는 유틸리티\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter  # 텍스트를 분할하는 도구"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrlcWNMyI0HQ",
        "outputId": "40c20284-e521-4e54-f4bf-55184b3f9d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "openai_api_key = userdata.get('openai_api_key')\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=openai_api_key) # OpenAI 모델 초기화 (by langchain)"
      ],
      "metadata": {
        "id": "gdjtGqsgI2Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 크롤링시 적절한 class_를 찾는 방법\n",
        "1. 브라우저에서 웹페이지 소스 확인:\n",
        "  - 개발자도구 > 해당 HTML 태그와 클래스 이름을 확인.\n",
        "\n",
        "\n",
        "2. BeautifulSoup로 임시 파싱 후 클래스 확인:\n",
        "  - 크롤링한 페이지를 BeautifulSoup로 로드하고 find_all() 메서드로 태그와 클래스 이름 확인."
      ],
      "metadata": {
        "id": "NS8gW4OPoIy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "WebBaseLoader는 LangChain이 제공하는 유틸리티로, 웹 기반 데이터를 로드하는 데 사용된다.\n",
        "\n",
        "주요 특징:\n",
        "- BeautifulSoup을 사용해 HTML 콘텐츠를 파싱한다.\n",
        "- 클래스 이름이나 태그 등으로 필요한 HTML 요소만 선택적으로 로드할 수 있다.\n",
        "- web_paths 매개변수에 하나 이상의 URL을 지정해 다양한 페이지에서 데이터를 가져올 수 있다.\n",
        "\"\"\"\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),  # 크롤링할 웹페이지 URL\n",
        "    bs_kwargs=dict(  # BeautifulSoup로 원하는 HTML 요소를 추출하는 설정\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")  # 주의. 크롤링 대상 웹문서의 구조에 맞춰서 설정해야 함.\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load() # 문서 데이터 불러오기 (크롤링 실행)"
      ],
      "metadata": {
        "id": "qhuzvEYGI4Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieval 정확도 향상을 위해 적절한 크기로 Text 분할하기\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "# 로드한 문서를 분할\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# 벡터 저장소 생성 (문서를 임베딩 후 Chroma에 저장)\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,  # 분할된 문서\n",
        "    embedding=OpenAIEmbeddings(api_key=openai_api_key)  # OpenAI 임베딩 사용\n",
        ")\n",
        "\n",
        "print(splits[0], '\\n\\n===============\\n===============\\n\\n', splits[1])"
      ],
      "metadata": {
        "id": "ewqNBMS_I7zB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a230557-61ce-4d16-9e43-b3a9454f0d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview#\n",
            "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
            "\n",
            "Planning\n",
            "\n",
            "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
            "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
            "\n",
            "\n",
            "Memory' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'} \n",
            "\n",
            "===============\n",
            "===============\n",
            "\n",
            " page_content='Memory\n",
            "\n",
            "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
            "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
            "\n",
            "\n",
            "Tool use\n",
            "\n",
            "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 랭체인의 invoke() 함수\n",
        "- invoke() 함수는 LangChain의 주요 작업을 수행하는 핵심 메서드 중 하나.\n",
        "- invoke는 그대로 번역하면 실행한다는 뜻. 어떠한 대상을 실행하는 기능이다.\n",
        "\n",
        "- 즉, 주어진 입력(프롬프트, 쿼리 등)을 기반으로 모델 또는 리소스에서 작업을 수행하고, 결과를 반환한다.\n",
        "  - LLM: 입력된 텍스트를 기반으로 AI 모델이 응답을 생성.\n",
        "  - Retrievers: 사용자의 입력 쿼리를 기반으로 검색 작업 수행.\n",
        "  - Prompts: 특정 컨텍스트와 질문을 기반으로 프롬프트 생성.\n",
        "\n",
        "\n",
        "# Prompt 구조\n",
        "\n",
        "최종 prompt 출력 값을 보면, \"**(RAG 사전 prompt) + (Context) + (유저 prompt)**'로 구성되어있다.\n",
        "\n",
        "유저 질문이 바뀌면 vectorDB에서 레퍼런스 삼을 문서도 다른 걸 검색하기 때문에,\n",
        "관련 document 내용 prompt도 바뀐다."
      ],
      "metadata": {
        "id": "2w-atszvl7x2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 벡터 저장소에서 검색 가능한 형태의 검색자 초기화\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# 검색된 문서 포맷팅\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "prompt = hub.pull(\"rlm/rag-prompt\") # 랭체인 허브에서 사전 정의된 RAG용 프롬프트를 가져옴\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKwUiRrPJMq1",
        "outputId": "cdf2c868-493c-47d3-cb79-34105efe68d4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_msg = \"What is the Self-Reflection?\" # 유저 질문\n",
        "retrieved_docs = retriever.invoke(user_msg) # 유저 질문 기반으로 문서 검색\n",
        "\n",
        "# 검색된 문서를 컨텍스트로 활용하여 사용자 질문을 포함한 프롬프트 생성\n",
        "user_prompt = prompt.invoke({\"context\": format_docs(retrieved_docs), \"question\": user_msg})\n",
        "print(user_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Owkx7RXWtEdV",
        "outputId": "7469faf3-9738-4231-d8fb-46f069ccb84d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: What is the Self-Reflection? \\nContext: Fig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM. \\nAnswer:\", additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 출력 결과\n",
        "- 해당 url에 있는 Self-reflection 에 대해서 잘 설명하고 있다."
      ],
      "metadata": {
        "id": "f8HyD1Goqezi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(user_prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqHKsIyUJc_m",
        "outputId": "775fefe5-1daa-4172-8ec9-bd88c0aad396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-reflection involves presenting two-shot examples to a Large Language Model (LLM), where each example consists of a failed trajectory and an ideal reflection for improving future plans. Reflections from these examples are then integrated into the agent's working memory, facilitating better context for subsequent queries. This process aims to guide the agent in avoiding past mistakes and enhancing planning efficiency.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문서에 없는 내용을 요청한 경우\n",
        "- 랭체인 사전 Prompt에는 레퍼런스에 없는 것에 대해서는 '모른다'고 답하라고 명시되어 있다.\n",
        "- 해당 문서에는 없는 '일론 머스크는 누구인가?' 에 대해서 어떻게 답변할까?\n",
        "\n",
        "<br></br>\n",
        "\n",
        "### 우선, VectorDB 에서 Context 선정부터 정상 작동하지 않는다.\n",
        "문서에 있는 Task Decomposition, Self-Reflection 에 대해서 물어봤을 때는 이를 기반으로 벡터 검색을 수행하기 떄문에 연관 내용을 가져왔었다.\n",
        "<br></br>\n",
        "그러나 Elon Musk에 대해 물어보자 관련 없는 내용이 선택됐다. 아마 그나마 벡터 유사도가 가장 높은 게 선정되었나보다.\n",
        "\n",
        "- Task Decomposition: Task decomposition에 대한 세부 정보를 포함한 문서가 선택됨.\n",
        "- Self-Reflection: Self-reflection과 관련된 Reflexion 프레임워크 문서가 선택됨.\n",
        "- Elon Musk: 엘론 머스크와 관련 없는 내용(동물의 도구 사용)이 선택됨 → 검색 결과가 적절하지 않음.\n",
        "\n",
        "<br></br>\n",
        "그 이유는,\n",
        "VectorDB는 사용자의 쿼리와 데이터베이스에 저장된 문서(또는 텍스트 청크)의 벡터 표현 간 유사도를 계산하여 가장 관련성 높은 문서를 반환하기 때문이다. 따라서, 검색 과정은 기본적으로 쿼리와 문서 간의 의미적 유사성에 의존한다."
      ],
      "metadata": {
        "id": "SvqI_1lptSG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_msg = \"Who is the Elon Musk?\"\n",
        "retrieved_docs = retriever.invoke(user_msg)\n",
        "\n",
        "user_prompt = prompt.invoke({\"context\": format_docs(retrieved_docs), \"question\": user_msg})\n",
        "print(user_prompt)"
      ],
      "metadata": {
        "id": "pSXxCzCNJfIO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55029f44-4e80-4711-e621-60f382011e32"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: Who is the Elon Musk? \\nContext: Fig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API). \\nAnswer:\", additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문서에 없는 내용 요청 - 출력 결과\n",
        "모델은 \"I don't know.\" 라고 솔직하게 답변한 것을 확인할 수 있다."
      ],
      "metadata": {
        "id": "TS0pfF4Dtoxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(user_prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Wg7JKvKtp7e",
        "outputId": "7ad1d950-7dd1-4b92-fb96-fbfb07e950b8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know.\n"
          ]
        }
      ]
    }
  ]
}